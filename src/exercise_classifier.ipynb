{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAiePlVM47y4"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dropout, LSTM, Dense, BatchNormalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEODL0PZmMO-"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0kan76wwuE9"
      },
      "source": [
        "#ModelTrainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQ7USZNmsnVy"
      },
      "outputs": [],
      "source": [
        "def load_data(data_dir):\n",
        "    print(\"Loading preprocessed data...\")\n",
        "    X = np.load(os.path.join(data_dir, 'X.npy'))\n",
        "    y = np.load(os.path.join(data_dir, 'y.npy'))\n",
        "    confidences = np.load(os.path.join(data_dir, 'confidences.npy'))\n",
        "    class_mapping = np.load(os.path.join(data_dir, 'class_mapping.npy'), allow_pickle=True).item()\n",
        "\n",
        "    print(f\"Loaded {len(X)} samples\")\n",
        "    print(f\"Input shape: {X.shape}\")\n",
        "    print(\"\\nClass distribution:\")\n",
        "    for class_name, class_idx in class_mapping.items():\n",
        "        total = sum(1 for label in y if label == class_idx)\n",
        "        print(f\"{class_name}: {total} samples\")\n",
        "\n",
        "    return X, y, confidences, class_mapping\n",
        "\n",
        "def build_model(input_shape, num_classes):\n",
        "    model = tf.keras.Sequential([\n",
        "        Conv1D(64, kernel_size=3, padding='same', activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        Dropout(0.35),\n",
        "        LSTM(128, return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.35),\n",
        "        LSTM(64, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.35),\n",
        "        Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(0.0003),\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    model.build(input_shape=(None,) + input_shape)\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "def get_callbacks():\n",
        "    class OverfittingDetectionCallback(tf.keras.callbacks.Callback):\n",
        "        def on_epoch_end(self, epoch, logs=None):\n",
        "            if logs.get('accuracy') - logs.get('val_accuracy') > 0.2:\n",
        "                print(\"\\nWarning: Possible overfitting detected!\")\n",
        "    return [\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6),\n",
        "        OverfittingDetectionCallback()\n",
        "    ]\n",
        "\n",
        "def calculate_class_weights(y_train):\n",
        "    class_weights = {}\n",
        "    unique, counts = np.unique(y_train, return_counts=True)\n",
        "    max_count = np.max(counts)\n",
        "    for c, count in zip(unique, counts):\n",
        "        class_weights[c] = max_count / count\n",
        "    return class_weights\n",
        "\n",
        "def smooth_curve(points, factor=0.8):\n",
        "    smoothed_points = []\n",
        "    for point in points:\n",
        "        if smoothed_points:\n",
        "            previous = smoothed_points[-1]\n",
        "            smoothed_points.append(previous * factor + point * (1 - factor))\n",
        "        else:\n",
        "            smoothed_points.append(point)\n",
        "    return smoothed_points\n",
        "\n",
        "def train_with_kfold(X, y, class_mapping, epochs=100, batch_size=16, n_splits=5, save_path=\"/content/drive/MyDrive/k_fold_CNN_LSTM_landmark\"):\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "    print(f\"\\nStarting {n_splits}-fold stratified cross-validation...\")\n",
        "    kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "    fold_results = []\n",
        "    fold_histories = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X, y)):\n",
        "        print(f\"\\nTraining Fold {fold+1}/{n_splits}\")\n",
        "        X_train, X_val = X[train_idx], X[val_idx]\n",
        "        y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "        model = build_model((X_train.shape[1], X_train.shape[2]), len(class_mapping))\n",
        "\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_data=(X_val, y_val),\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            callbacks=get_callbacks(),\n",
        "            class_weight=calculate_class_weights(y_train),\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        y_pred = model.predict(X_val)\n",
        "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "        f1 = f1_score(y_val, y_pred_classes, average='weighted', zero_division=0)\n",
        "\n",
        "        cm = confusion_matrix(y_val, y_pred_classes, normalize='true')\n",
        "\n",
        "        fold_results.append({\n",
        "            'val_accuracy': history.history['val_accuracy'][-1],\n",
        "            'val_loss': history.history['val_loss'][-1],\n",
        "            'f1_score': f1,\n",
        "            'confusion_matrix': cm,\n",
        "            'classification_report': classification_report(\n",
        "                y_val, y_pred_classes,\n",
        "                target_names=list(class_mapping.keys()),\n",
        "                output_dict=True, zero_division=0),\n",
        "            'y_true': y_val,\n",
        "            'y_pred': y_pred_classes\n",
        "        })\n",
        "        fold_histories.append(history.history)\n",
        "        model.save(os.path.join(save_path, f'model_fold_{fold+1}.keras'))\n",
        "\n",
        "    save_training_plots(fold_histories, save_path)\n",
        "    save_fold_results(fold_results, fold_histories, save_path)\n",
        "    return fold_results, fold_histories\n",
        "\n",
        "def save_training_plots(fold_histories, save_path):\n",
        "    os.makedirs(os.path.join(save_path, 'training_plots'), exist_ok=True)\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    for h in fold_histories:\n",
        "        plt.plot(smooth_curve(h['accuracy']), label='Train', alpha=0.5)\n",
        "        plt.plot(smooth_curve(h['val_accuracy']), label='Val', alpha=0.5)\n",
        "    plt.title('Smoothed Accuracy Across Folds')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    for h in fold_histories:\n",
        "        plt.plot(smooth_curve(h['loss']), label='Train', alpha=0.5)\n",
        "        plt.plot(smooth_curve(h['val_loss']), label='Val', alpha=0.5)\n",
        "    plt.title('Smoothed Loss Across Folds')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(save_path, 'training_plots', 'combined_training_curves.png'), dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "def save_fold_results(fold_results, fold_histories, save_path):\n",
        "    metrics = []\n",
        "    for i, (result, history) in enumerate(zip(fold_results, fold_histories)):\n",
        "        fold_dir = os.path.join(save_path, 'fold_' + str(i+1))\n",
        "        os.makedirs(fold_dir, exist_ok=True)\n",
        "\n",
        "        # Confusion matrix\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        sns.heatmap(result['confusion_matrix'], annot=True, fmt=\".2f\", cmap='Blues',\n",
        "                    xticklabels=list(class_mapping.keys()),\n",
        "                    yticklabels=list(class_mapping.keys()))\n",
        "        plt.title(f'Normalized Confusion Matrix - Fold {i+1}')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(fold_dir, 'confusion_matrix.png'), dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "        pd.DataFrame(result['classification_report']).transpose().to_csv(os.path.join(fold_dir, 'classification_report.csv'))\n",
        "        metrics.append({\n",
        "            'fold': i+1,\n",
        "            'val_accuracy': result['val_accuracy'],\n",
        "            'val_loss': result['val_loss'],\n",
        "            'f1_score': result['f1_score'],\n",
        "            'epochs': len(history['loss'])\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(metrics)\n",
        "    df.to_csv(os.path.join(save_path, 'all_fold_metrics.csv'), index=False)\n",
        "\n",
        "    avg = df.mean()\n",
        "    pd.DataFrame([avg]).to_csv(os.path.join(save_path, 'average_metrics.csv'), index=False)\n",
        "\n",
        "    print(\"\\nK-Fold Summary:\")\n",
        "    print(df.describe())\n",
        "\n",
        "# Entry point for Colab\n",
        "if __name__ == \"__main__\":\n",
        "    data_dir = '/content/drive/MyDrive/CVP/processed_landmark_data'\n",
        "    save_path = \"/content/drive/MyDrive/CVP/k_fold_CNN_LSTM_landmark\"\n",
        "    X, y, confidences, class_mapping = load_data(data_dir)\n",
        "    train_with_kfold(X, y, class_mapping, epochs=100, batch_size=16, n_splits=5, save_path=save_path)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
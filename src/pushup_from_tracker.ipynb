{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "192cb734-c242-498b-9694-7b521918154f",
      "metadata": {
        "id": "192cb734-c242-498b-9694-7b521918154f"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# Load YOLOv8n-pose model\n",
        "model = YOLO('yolov8n-pose.pt')\n",
        "\n",
        "# Paths\n",
        "data_root = r'C:/Users/Kassimi/OneDrive/Bureau/cv_data/pushups'\n",
        "folders = ['correct sequence', 'wrong sequence']\n",
        "augmented_folders = {'correct sequence': 'aug_correct', 'wrong sequence': 'aug_wrong'}\n",
        "\n",
        "# Create folders for augmented videos\n",
        "for folder in augmented_folders.values():\n",
        "    os.makedirs(os.path.join(data_root, folder), exist_ok=True)\n",
        "\n",
        "# Augmentation function\n",
        "def augment_frame(frame):\n",
        "    if random.random() < 0.5:\n",
        "        frame = cv2.flip(frame, 1)  # Horizontal flip\n",
        "    alpha = random.uniform(0.8, 1.2)  # Contrast\n",
        "    beta = random.randint(-30, 30)   # Brightness\n",
        "    frame = cv2.convertScaleAbs(frame, alpha=alpha, beta=beta)\n",
        "    return frame\n",
        "\n",
        "#  Create augmented videos\n",
        "print(\"ðŸŽ¥ Augmenting and saving videos...\")\n",
        "for label in folders:\n",
        "    input_folder = os.path.join(data_root, label)\n",
        "    output_folder = os.path.join(data_root, augmented_folders[label])\n",
        "\n",
        "    for video_file in os.listdir(input_folder):\n",
        "        if not video_file.endswith(('.mp4', '.avi', '.mov')):\n",
        "            continue\n",
        "\n",
        "        input_path = os.path.join(input_folder, video_file)\n",
        "        output_path = os.path.join(output_folder, f\"aug_{video_file}\")\n",
        "\n",
        "        cap = cv2.VideoCapture(input_path)\n",
        "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            aug_frame = augment_frame(frame)\n",
        "            out.write(aug_frame)\n",
        "\n",
        "        cap.release()\n",
        "        out.release()\n",
        "\n",
        "print(\"âœ… Augmented videos saved.\\n\")\n",
        "\n",
        "#  Extract frames and keypoints from both original and augmented videos\n",
        "print(\"ðŸ“¸ Extracting frames and pose keypoints...\")\n",
        "\n",
        "output_data = []\n",
        "\n",
        "# Process both original and augmented videos\n",
        "for label in folders + list(augmented_folders.values()):  # Including augmented folders\n",
        "    folder_path = os.path.join(data_root, label)\n",
        "\n",
        "    for video_file in os.listdir(folder_path):\n",
        "        if not video_file.endswith(('.mp4', '.avi', '.mov')):\n",
        "            continue\n",
        "\n",
        "        video_path = os.path.join(folder_path, video_file)\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        frame_index = 0\n",
        "\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            # Pose estimation\n",
        "            results = model.predict(frame, save=False, verbose=False)\n",
        "            keypoints = results[0].keypoints.xy.cpu().numpy()\n",
        "\n",
        "            if len(keypoints) == 0:\n",
        "                frame_index += 1\n",
        "                continue\n",
        "\n",
        "            # Take only the first person detected\n",
        "            person_keypoints = keypoints[0].flatten()  # shape: (34,)\n",
        "            row = [video_file, frame_index] + list(person_keypoints) + [label]\n",
        "            output_data.append(row)\n",
        "\n",
        "            frame_index += 1\n",
        "        cap.release()\n",
        "\n",
        "# Save to CSV\n",
        "columns = ['video_id', 'frame'] + [f'kp_{i}_{coord}' for i in range(17) for coord in ['x', 'y']] + ['label']\n",
        "df = pd.DataFrame(output_data, columns=columns)\n",
        "\n",
        "output_csv_path = os.path.join(data_root, 'pushup_pose_data.csv')\n",
        "df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "print(f\"âœ… Keypoints from all videos (original + augmented) saved to: {output_csv_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b1d4ccf-97b8-428c-907b-061f48a635ad",
      "metadata": {
        "id": "2b1d4ccf-97b8-428c-907b-061f48a635ad"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Load CSV\n",
        "df = pd.read_csv(r\"C:/Users/Kassimi/OneDrive/Bureau/cv_data/pushups\\pushup_pose_datav0.csv\")\n",
        "\n",
        "# Convert label to numeric\n",
        "df['label'] = df['label'].map({'correct sequence': 1, 'wrong sequence': 0})\n",
        "\n",
        "# Parameters\n",
        "sequence_length = 30  # Choose fixed sequence length\n",
        "min_frames_required = sequence_length\n",
        "\n",
        "# Normalize keypoints\n",
        "keypoint_cols = [col for col in df.columns if col.startswith('kp_')]\n",
        "scaler = StandardScaler()\n",
        "df[keypoint_cols] = scaler.fit_transform(df[keypoint_cols])\n",
        "\n",
        "# Group by video and build sequences\n",
        "X_sequences = []\n",
        "y_labels = []\n",
        "for video_id, group in df.groupby('video_id'):\n",
        "    group = group.sort_values('frame')\n",
        "    keypoints = group[keypoint_cols].values  # shape: (num_frames, 34)\n",
        "    label = group['label'].iloc[0]\n",
        "\n",
        "    # Skip short sequences\n",
        "    if len(keypoints) < min_frames_required:\n",
        "        continue\n",
        "\n",
        "    # Break long videos into multiple sequences\n",
        "    for start in range(0, len(keypoints) - sequence_length + 1, sequence_length):\n",
        "        seq = keypoints[start:start + sequence_length]\n",
        "        X_sequences.append(seq)\n",
        "        y_labels.append(label)\n",
        "\n",
        "# Convert to tensors\n",
        "X_tensor = torch.tensor(np.array(X_sequences), dtype=torch.float32)  # shape: (N, seq_len, 34)\n",
        "y_tensor = torch.tensor(y_labels, dtype=torch.long)\n",
        "\n",
        "# Train/val split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
        "\n",
        "# PyTorch Dataset\n",
        "class PushupSequenceDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# Dataloaders\n",
        "train_dataset = PushupSequenceDataset(X_train, y_train)\n",
        "val_dataset = PushupSequenceDataset(X_val, y_val)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4415e151-74bd-4fcc-9577-db14fe9e4d5e",
      "metadata": {
        "id": "4415e151-74bd-4fcc-9577-db14fe9e4d5e"
      },
      "outputs": [],
      "source": [
        "class PushupLSTM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PushupLSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size=34, hidden_size=64, num_layers=1, batch_first=True)\n",
        "        self.fc1 = nn.Linear(64, 32)  # LSTM output to hidden layer\n",
        "        self.fc2 = nn.Linear(32, 2)   # Hidden layer to output (2 units for binary classification)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, sequence_length, 34)\n",
        "        lstm_out, _ = self.lstm(x)              # Output shape: (batch_size, sequence_length, hidden_size)\n",
        "        x = lstm_out[:, -1, :]                  # Take output from the last time step\n",
        "        x = self.relu(self.fc1(x))              # Pass through FC layer\n",
        "        x = self.fc2(x)                         # Output layer (2 values for binary classification)\n",
        "        return x                                # Return raw logits for CrossEntropyLoss\n",
        "\n",
        "# Initialize the model\n",
        "model = PushupLSTM()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9374d71e-142d-4ce6-b694-d6ca743b418e",
      "metadata": {
        "id": "9374d71e-142d-4ce6-b694-d6ca743b418e"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)  # Shape: (batch_size, 2)\n",
        "\n",
        "        # Now calculate the loss - no need for shape adjustment\n",
        "        loss = criterion(outputs, labels)  # labels should be of shape [batch_size]\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)  # Get the index of the maximum logit for prediction\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_train_loss = train_loss / total\n",
        "    train_acc = correct / total\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_val_loss = val_loss / total\n",
        "    val_acc = correct / total\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
        "          f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
        "          f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'pushup_classifier.pth')\n",
        "\n",
        "# Example of how to use the model for inference\n",
        "def predict_sequence(model, sequence):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        sequence = sequence.to(device)\n",
        "        output = model(sequence)\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        return predicted.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1303bdc4-265a-4385-a33b-d8ac0ba3e78d",
      "metadata": {
        "id": "1303bdc4-265a-4385-a33b-d8ac0ba3e78d"
      },
      "outputs": [],
      "source": [
        "# Save the trained model\n",
        "model_path = r\"C:\\Users\\Kassimi\\OneDrive\\Bureau\\cv_data\\augmented_pushup_lstm_modelv1.pth\"\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"âœ… Model saved to {model_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "578af7fb-cf29-40ca-86e1-19ef5aa3c36d",
      "metadata": {
        "id": "578af7fb-cf29-40ca-86e1-19ef5aa3c36d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from collections import deque\n",
        "\n",
        "# Define LSTM model class (needs to match what you used in training)\n",
        "class PushupLSTM(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PushupLSTM, self).__init__()\n",
        "        self.lstm = torch.nn.LSTM(input_size=34, hidden_size=64, num_layers=1, batch_first=True)\n",
        "        self.fc1 = torch.nn.Linear(64, 32)\n",
        "        self.fc2 = torch.nn.Linear(32, 2)  # 2 outputs for binary classification\n",
        "        self.relu = torch.nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        x = lstm_out[:, -1, :]  # Take output from the last time step\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# LSTM sequence length (must match training)\n",
        "sequence_length = 30\n",
        "\n",
        "# Load trained model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_path = r'C:\\Users\\Kassimi\\OneDrive\\Bureau\\cv_data\\augmented_pushup_lstm_modelv1.pth'\n",
        "model = PushupLSTM()\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Load scaler (refit on original dataset)\n",
        "try:\n",
        "    df = pd.read_csv(r\"C:\\Users\\Kassimi\\OneDrive\\Bureau\\cv_data\\pushups\\pushup_pose_datav0.csv\")\n",
        "    # Only use keypoint columns that contain numerical data\n",
        "    keypoint_cols = [col for col in df.columns if col.startswith('kp_')]\n",
        "\n",
        "    # Make sure we only use numeric data for scaling\n",
        "    # First check if the data is clean\n",
        "    for col in keypoint_cols:\n",
        "        if df[col].dtype == 'object':\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    # Drop rows with NaN values that might have been created by coercion\n",
        "    df = df.dropna(subset=keypoint_cols)\n",
        "\n",
        "    # Now fit the scaler\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(df[keypoint_cols])\n",
        "    print(f\"Successfully fit scaler on {len(df)} rows with {len(keypoint_cols)} keypoint features\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error fitting scaler: {e}\")\n",
        "    print(\"Will continue with default scaling instead\")\n",
        "    # If loading fails, we'll use a simple normalization as fallback\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "# Load YOLO pose model\n",
        "try:\n",
        "    yolo_model = YOLO('yolov8n-pose.pt')\n",
        "    print(\"YOLO pose model loaded successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading YOLO model: {e}\")\n",
        "    exit(1)\n",
        "\n",
        "# Video path\n",
        "video_path = r\"D:\\Kassimi\\Pictures\\Camera Roll\\WIN_20250505_09_38_56_Pro.mp4\"\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "if not cap.isOpened():\n",
        "    print(f\"Error: Could not open video {video_path}\")\n",
        "    exit(1)\n",
        "\n",
        "print(f\"Video loaded successfully. Starting prediction...\")\n",
        "\n",
        "# Buffer to store sequence of keypoints\n",
        "sequence_buffer = deque(maxlen=sequence_length)\n",
        "\n",
        "# For video output\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "out = cv2.VideoWriter('pushup_prediction3.mp4', cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
        "\n",
        "frame_count = 0\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    frame_count += 1\n",
        "    if frame_count % 10 == 0:\n",
        "        print(f\"Processing frame {frame_count}\")\n",
        "\n",
        "    # Get YOLO pose estimation\n",
        "    results = yolo_model.predict(frame, save=False, verbose=False)\n",
        "\n",
        "    # Draw the pose on the frame (optional)\n",
        "    annotated_frame = results[0].plot()\n",
        "\n",
        "    # Check if any person was detected\n",
        "    keypoints = results[0].keypoints.xy.cpu().numpy()\n",
        "\n",
        "    if len(keypoints) > 0:  # If at least one person is detected\n",
        "        # Take the first person's keypoints\n",
        "        person_keypoints = keypoints[0].flatten()\n",
        "\n",
        "        # Make sure we have the right number of keypoints\n",
        "        if len(person_keypoints) == 34:  # 17 keypoints * 2 (x,y)\n",
        "            try:\n",
        "                # Scale keypoints using our scaler\n",
        "                if hasattr(scaler, 'mean_'):  # Check if scaler was properly fitted\n",
        "                    person_keypoints_scaled = scaler.transform([person_keypoints])[0]\n",
        "                else:\n",
        "                    # Simple normalization as fallback\n",
        "                    person_keypoints_scaled = (person_keypoints - np.mean(person_keypoints)) / (np.std(person_keypoints) + 1e-8)\n",
        "\n",
        "                # Add to sequence buffer\n",
        "                sequence_buffer.append(person_keypoints_scaled)\n",
        "\n",
        "                # Only predict when we have a full sequence\n",
        "                if len(sequence_buffer) == sequence_length:\n",
        "                    input_seq = torch.tensor([list(sequence_buffer)], dtype=torch.float32).to(device)\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        output = model(input_seq)\n",
        "                        probabilities = torch.softmax(output, dim=1)\n",
        "                        confidence, predicted_class = torch.max(probabilities, 1)\n",
        "\n",
        "                    # Get the prediction and confidence\n",
        "                    label = 'Correct Form' if predicted_class.item() == 1 else 'Incorrect Form'\n",
        "                    conf_value = confidence.item()\n",
        "\n",
        "                    # Set color based on prediction (green for correct, red for incorrect)\n",
        "                    color = (0, 255, 0) if label == 'Correct Form' else (0, 0, 255)\n",
        "\n",
        "                    # Display prediction on frame\n",
        "                    cv2.putText(annotated_frame, f\"{label} ({conf_value:.2f})\",\n",
        "                                (30, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.0, color, 2)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in prediction: {e}\")\n",
        "\n",
        "    # Display frame with predictions\n",
        "    cv2.imshow('Pushup Form Prediction', annotated_frame)\n",
        "    out.write(annotated_frame)\n",
        "\n",
        "    # Break loop if 'q' is pressed\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# Release resources\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n",
        "print(\"Prediction completed. Video saved as 'pushup_prediction2.mp4'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5264be01-f65e-4ec7-9711-7497cdbcce83",
      "metadata": {
        "id": "5264be01-f65e-4ec7-9711-7497cdbcce83"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from collections import deque\n",
        "import time\n",
        "\n",
        "# Define LSTM model class (same as your original)\n",
        "class PushupLSTM(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PushupLSTM, self).__init__()\n",
        "        self.lstm = torch.nn.LSTM(input_size=34, hidden_size=64, num_layers=1, batch_first=True)\n",
        "        self.fc1 = torch.nn.Linear(64, 32)\n",
        "        self.fc2 = torch.nn.Linear(32, 2)  # 2 outputs for binary classification\n",
        "        self.relu = torch.nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        x = lstm_out[:, -1, :]  # Take output from the last time step\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# LSTM sequence length (must match training)\n",
        "sequence_length = 30\n",
        "\n",
        "# Load trained model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Update the path to your model\n",
        "model_path = r'C:\\Users\\Kassimi\\OneDrive\\Bureau\\cv_data\\augmented_pushup_lstm_modelv1.pth'\n",
        "model = PushupLSTM()\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "print(\"LSTM model loaded successfully\")\n",
        "\n",
        "# Create a simple scaler as fallback since we might not have access to the original dataset\n",
        "scaler = StandardScaler()\n",
        "print(\"Using default scaler (make sure your keypoint values are properly scaled)\")\n",
        "\n",
        "# Load YOLO pose model\n",
        "try:\n",
        "    yolo_model = YOLO('yolov8n-pose.pt')\n",
        "    print(\"YOLO pose model loaded successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading YOLO model: {e}\")\n",
        "    exit(1)\n",
        "\n",
        "# Initialize webcam\n",
        "cap = cv2.VideoCapture(0)  # 0 is usually the default webcam\n",
        "if not cap.isOpened():\n",
        "    print(\"Error: Could not open webcam\")\n",
        "    exit(1)\n",
        "\n",
        "# Set webcam resolution (optional)\n",
        "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
        "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
        "\n",
        "print(\"Webcam initialized. Starting live prediction...\")\n",
        "print(\"Press 'q' to quit\")\n",
        "\n",
        "# Buffer to store sequence of keypoints\n",
        "sequence_buffer = deque(maxlen=sequence_length)\n",
        "\n",
        "# Variable to track prediction status\n",
        "prediction_ready = False\n",
        "last_prediction = \"Waiting for enough frames...\"\n",
        "confidence = 0.0\n",
        "color = (255, 255, 0)  # Yellow for waiting\n",
        "\n",
        "# For FPS calculation\n",
        "prev_time = time.time()\n",
        "fps = 0\n",
        "\n",
        "# Flag to toggle recording\n",
        "is_recording = False\n",
        "out = None\n",
        "\n",
        "# Counter for pushups\n",
        "pushup_count = 0\n",
        "last_state = None\n",
        "pushup_threshold = 0.7  # Confidence threshold for counting\n",
        "\n",
        "frame_count = 0\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        print(\"Failed to grab frame from webcam\")\n",
        "        break\n",
        "\n",
        "    # Calculate FPS\n",
        "    current_time = time.time()\n",
        "    fps = 1 / (current_time - prev_time)\n",
        "    prev_time = current_time\n",
        "\n",
        "    # Process every frame\n",
        "    frame_count += 1\n",
        "\n",
        "    # Get YOLO pose estimation\n",
        "    results = yolo_model.predict(frame, save=False, verbose=False)\n",
        "\n",
        "    # Draw the pose on the frame\n",
        "    annotated_frame = results[0].plot()\n",
        "\n",
        "    # Check if any person was detected\n",
        "    keypoints = results[0].keypoints.xy.cpu().numpy()\n",
        "\n",
        "    if len(keypoints) > 0:  # If at least one person is detected\n",
        "        # Take the first person's keypoints\n",
        "        person_keypoints = keypoints[0].flatten()\n",
        "\n",
        "        # Make sure we have the right number of keypoints\n",
        "        if len(person_keypoints) == 34:  # 17 keypoints * 2 (x,y)\n",
        "            try:\n",
        "                # Simple normalization as we don't have the original scaler\n",
        "                person_keypoints_scaled = (person_keypoints - np.mean(person_keypoints)) / (np.std(person_keypoints) + 1e-8)\n",
        "\n",
        "                # Add to sequence buffer\n",
        "                sequence_buffer.append(person_keypoints_scaled)\n",
        "\n",
        "                # Only predict when we have a full sequence\n",
        "                if len(sequence_buffer) == sequence_length:\n",
        "                    input_seq = torch.tensor([list(sequence_buffer)], dtype=torch.float32).to(device)\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        output = model(input_seq)\n",
        "                        probabilities = torch.softmax(output, dim=1)\n",
        "                        confidence_tensor, predicted_class = torch.max(probabilities, 1)\n",
        "\n",
        "                    # Get the prediction and confidence\n",
        "                    prediction_ready = True\n",
        "                    last_prediction = 'Correct Form' if predicted_class.item() == 1 else 'Incorrect Form'\n",
        "                    confidence = confidence_tensor.item()\n",
        "\n",
        "                    # Set color based on prediction\n",
        "                    color = (0, 255, 0) if last_prediction == 'Correct Form' else (0, 0, 255)\n",
        "\n",
        "                    # Simple pushup counter logic\n",
        "                    current_state = 'up' if confidence > pushup_threshold and last_prediction == 'Correct Form' else 'down'\n",
        "                    if last_state == 'down' and current_state == 'up':\n",
        "                        pushup_count += 1\n",
        "                    last_state = current_state\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in prediction: {e}\")\n",
        "\n",
        "    # Display prediction on frame\n",
        "    if prediction_ready:\n",
        "        cv2.putText(annotated_frame, f\"{last_prediction} ({confidence:.2f})\",\n",
        "                    (30, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.0, color, 2)\n",
        "    else:\n",
        "        cv2.putText(annotated_frame, last_prediction,\n",
        "                    (30, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.0, color, 2)\n",
        "\n",
        "    # Display pushup counter\n",
        "    cv2.putText(annotated_frame, f\"Pushup Count: {pushup_count}\",\n",
        "                (30, 90), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 165, 0), 2)\n",
        "\n",
        "    # Display FPS\n",
        "    cv2.putText(annotated_frame, f\"FPS: {fps:.1f}\",\n",
        "                (frame.shape[1] - 150, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
        "\n",
        "    # Display recording status if recording\n",
        "    if is_recording:\n",
        "        cv2.putText(annotated_frame, \"REC\",\n",
        "                    (frame.shape[1] - 70, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
        "        # Record frame if we're recording\n",
        "        out.write(annotated_frame)\n",
        "\n",
        "    # Display frame with predictions\n",
        "    cv2.imshow('Pushup Form Detection (Webcam)', annotated_frame)\n",
        "\n",
        "    # Check for key presses\n",
        "    key = cv2.waitKey(1) & 0xFF\n",
        "\n",
        "    # Quit if 'q' is pressed\n",
        "    if key == ord('q'):\n",
        "        break\n",
        "\n",
        "    # Toggle recording if 'r' is pressed\n",
        "    elif key == ord('r'):\n",
        "        is_recording = not is_recording\n",
        "        if is_recording:\n",
        "            # Initialize video writer\n",
        "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "            timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "            out = cv2.VideoWriter(f'pushup_webcam_{timestamp}.mp4',\n",
        "                                 fourcc, 20.0,\n",
        "                                 (frame.shape[1], frame.shape[0]))\n",
        "            print(\"Recording started\")\n",
        "        else:\n",
        "            if out is not None:\n",
        "                out.release()\n",
        "                print(\"Recording stopped and saved\")\n",
        "\n",
        "    # Reset pushup counter if 'c' is pressed\n",
        "    elif key == ord('c'):\n",
        "        pushup_count = 0\n",
        "        print(\"Pushup counter reset\")\n",
        "\n",
        "# Release resources\n",
        "cap.release()\n",
        "if out is not None:\n",
        "    out.release()\n",
        "cv2.destroyAllWindows()\n",
        "print(\"Application closed\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
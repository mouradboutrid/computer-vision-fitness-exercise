{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80c6cc9d-aec8-4b96-86e8-6a1f4fa51272",
      "metadata": {
        "id": "80c6cc9d-aec8-4b96-86e8-6a1f4fa51272"
      },
      "outputs": [],
      "source": [
        "pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c1e19f5-6fa9-4557-ab8a-ac38a8ec0a66",
      "metadata": {
        "id": "9c1e19f5-6fa9-4557-ab8a-ac38a8ec0a66"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolov8n-pose.pt')\n",
        "\n",
        "cap = cv2.VideoCapture(0)  #open webcam\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Run YOLO pose prediction WITHOUT showing\n",
        "    results = model.predict(frame, conf=0.5, stream=True)\n",
        "\n",
        "    for r in results:\n",
        "        # Get the annotated frame from results\n",
        "        annotated_frame = r.plot()\n",
        "\n",
        "        # Show using OpenCV\n",
        "        cv2.imshow('YOLOv8 Pose Detection', annotated_frame)\n",
        "\n",
        "    # Exit when pressing 'f'\n",
        "    if cv2.waitKey(1) & 0xFF == ord('f'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe4a91f8-99a5-4ad0-8e99-b028901d2d91",
      "metadata": {
        "id": "fe4a91f8-99a5-4ad0-8e99-b028901d2d91"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Load YOLOv8-pose model\n",
        "model = YOLO('yolov8n-pose.pt')\n",
        "\n",
        "\n",
        "data_root = r'C:/Users/Kassimi/OneDrive/Bureau/cv_data/squat/'\n",
        "folders = ['correct_form', 'wrong_form']\n",
        "\n",
        "# Output CSV data\n",
        "output_data = []\n",
        "\n",
        "for label in folders:\n",
        "    folder_path = os.path.join(data_root, label)\n",
        "\n",
        "    for video_file in os.listdir(folder_path):\n",
        "        if not video_file.endswith(('.mp4', '.avi', '.mov')):\n",
        "            continue\n",
        "\n",
        "        video_path = os.path.join(folder_path, video_file)\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        frame_index = 0\n",
        "\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            # Pose estimation\n",
        "            results = model.predict(frame, save=False, verbose=False)\n",
        "            keypoints = results[0].keypoints.xy.cpu().numpy()  # shape: (n_person, 17, 2)\n",
        "\n",
        "            if len(keypoints) == 0:\n",
        "                frame_index += 1\n",
        "                continue  # No person detected in frame\n",
        "\n",
        "            # Take only the first person detected\n",
        "            person_keypoints = keypoints[0].flatten()  # shape: (34,)\n",
        "\n",
        "            # Store with metadata\n",
        "            row = [video_file, frame_index] + list(person_keypoints) + [label]\n",
        "            output_data.append(row)\n",
        "\n",
        "            frame_index += 1\n",
        "\n",
        "        cap.release()\n",
        "\n",
        "# Define column names\n",
        "columns = ['video_id', 'frame'] + [f'kp_{i}_{coord}' for i in range(17) for coord in ['x', 'y']] + ['label']\n",
        "df = pd.DataFrame(output_data, columns=columns)\n",
        "\n",
        "# Save to CSV\n",
        "output_csv_path = os.path.join(data_root, 'squat_pose_data.csv')\n",
        "df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "print(f\"✅ Sequential keypoints saved to: {output_csv_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45733487-20c5-4886-8d58-8a4b283ca408",
      "metadata": {
        "id": "45733487-20c5-4886-8d58-8a4b283ca408"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Load the CSV data\n",
        "df = pd.read_csv(r\"C:\\Users\\Kassimi\\OneDrive\\Bureau\\cv_data\\squat\\squat_pose_data.csv\")\n",
        "\n",
        "# Features (keypoints) and labels\n",
        "X = df.drop(columns=['label'])  # All columns except label are keypoints\n",
        "y = df['label'].map({'correct': 1, 'wrong': 0})  # Convert labels to 1 (correct) or 0 (wrong)\n",
        "\n",
        "# Normalize keypoints using StandardScaler (so all values are on a similar scale)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Convert to tensors\n",
        "X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y.values, dtype=torch.long)\n",
        "\n",
        "# Train-test split (80% training, 20% validation)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a PyTorch dataset for easier batching\n",
        "class SquatDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]\n",
        "\n",
        "# Create DataLoader objects for batching\n",
        "train_dataset = SquatDataset(X_train, y_train)\n",
        "val_dataset = SquatDataset(X_val, y_val)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b56e5b26-95fd-466a-9dee-22148a57b078",
      "metadata": {
        "id": "b56e5b26-95fd-466a-9dee-22148a57b078"
      },
      "outputs": [],
      "source": [
        "class SquatNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SquatNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(34, 64)  # Input layer to hidden layer\n",
        "        self.fc2 = nn.Linear(64, 32)  # Hidden layer\n",
        "        self.fc3 = nn.Linear(32, 2)   # Output layer (correct or wrong)\n",
        "        self.relu = nn.ReLU()         # Activation function\n",
        "        self.softmax = nn.Softmax(dim=1)  # Softmax for output probabilities\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))  # First hidden layer\n",
        "        x = self.relu(self.fc2(x))  # Second hidden layer\n",
        "        x = self.fc3(x)             # Output layer\n",
        "        return self.softmax(x)      # Softmax activation for probabilities\n",
        "\n",
        "# Initialize the model\n",
        "model = SquatNet()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d03bdd0-dcb1-42cd-b273-341af8e60672",
      "metadata": {
        "id": "5d03bdd0-dcb1-42cd-b273-341af8e60672"
      },
      "outputs": [],
      "source": [
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()  # Used for multi-class classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "    correct_preds = 0\n",
        "    total_preds = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()   # Clear the gradients\n",
        "        outputs = model(inputs) # Get predictions from the model\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()         # Backpropagation\n",
        "        optimizer.step()        # Update model weights\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total_preds += labels.size(0)\n",
        "        correct_preds += (predicted == labels).sum().item()\n",
        "\n",
        "    # Calculate training accuracy and loss\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_accuracy = correct_preds / total_preds\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    correct_preds = 0\n",
        "    total_preds = 0\n",
        "    with torch.no_grad():  # No gradient calculation during evaluation\n",
        "        for inputs, labels in val_loader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_preds += labels.size(0)\n",
        "            correct_preds += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_accuracy = correct_preds / total_preds\n",
        "\n",
        "    # Print training and validation results\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
        "          f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy*100:.2f}%, \"\n",
        "          f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "059f279b-e87a-4f26-8c3b-edd2760a2893",
      "metadata": {
        "id": "059f279b-e87a-4f26-8c3b-edd2760a2893"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "torch.save(model.state_dict(), r'C:\\Users\\Kassimi\\OneDrive\\Bureau\\cv_data\\squatsquat_model.pth')\n",
        "print(\"Model saved!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abe4a10b-12eb-442f-b5cb-89eec5775f5e",
      "metadata": {
        "id": "abe4a10b-12eb-442f-b5cb-89eec5775f5e"
      },
      "outputs": [],
      "source": [
        "pip install --upgrade pytube\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f34cc6eb-8210-40d0-b201-a4025faed657",
      "metadata": {
        "id": "f34cc6eb-8210-40d0-b201-a4025faed657"
      },
      "source": [
        "now to making a prediction on a new video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "649b3db7-13d0-4af7-880c-c389166ae115",
      "metadata": {
        "id": "649b3db7-13d0-4af7-880c-c389166ae115"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "142259f2-a885-4c59-8b45-80a8c394c5fb",
      "metadata": {
        "id": "142259f2-a885-4c59-8b45-80a8c394c5fb"
      },
      "outputs": [],
      "source": [
        "# Load the trained model\n",
        "model = SquatNet()\n",
        "model.load_state_dict(torch.load(r'C:\\Users\\Kassimi\\OneDrive\\Bureau\\cv_data\\squatsquat_model.pth'))\n",
        "model.eval()\n",
        "\n",
        "# Load the scaler you used before\n",
        "scaler = StandardScaler()\n",
        "df = pd.read_csv(r\"C:\\Users\\Kassimi\\OneDrive\\Bureau\\cv_data\\squat\\squat_pose_data.csv\")\n",
        "X = df.drop(columns=['label'])\n",
        "scaler.fit(X)  # fit on original data to re-use for new data\n",
        "\n",
        "# Load YOLO pose model again\n",
        "yolo_model = YOLO('yolov8n-pose.pt')\n",
        "\n",
        "# New video path\n",
        "new_video_path = r\"C:\\Users\\Kassimi\\OneDrive\\Bureau\\cv_data\\squat\\correct_form\\correct_11.mp4\"\n",
        "cap = cv2.VideoCapture(new_video_path)\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Pose estimation\n",
        "    results = yolo_model.predict(frame, save=False, verbose=False)\n",
        "    keypoints = results[0].keypoints.xy.cpu().numpy()\n",
        "\n",
        "    if len(keypoints) == 0:\n",
        "        continue  # No person detected\n",
        "\n",
        "    # Only first person\n",
        "    person_keypoints = keypoints[0].flatten()\n",
        "\n",
        "    if len(person_keypoints) != 34:  # 17 keypoints * 2 (x,y)\n",
        "        continue  # Safety check\n",
        "\n",
        "    # Scale keypoints\n",
        "    person_keypoints_scaled = scaler.transform([person_keypoints])\n",
        "\n",
        "    # Convert to tensor\n",
        "    input_tensor = torch.tensor(person_keypoints_scaled, dtype=torch.float32)\n",
        "\n",
        "    # Predict\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "        _, predicted_class = torch.max(output, 1)\n",
        "\n",
        "    # Map prediction\n",
        "    label = 'Correct' if predicted_class.item() == 1 else 'Wrong'\n",
        "\n",
        "    # Display result on frame\n",
        "    cv2.putText(frame, label, (30, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0) if label == 'Correct' else (0, 0, 255), 3)\n",
        "\n",
        "    # Show frame\n",
        "    cv2.imshow('Squat Form Prediction', frame)\n",
        "\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b28af42-ed8e-4a85-a229-cc7ba1c7e500",
      "metadata": {
        "id": "2b28af42-ed8e-4a85-a229-cc7ba1c7e500"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Define SquatNet again\n",
        "class SquatNet(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SquatNet, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(34, 64)\n",
        "        self.fc2 = torch.nn.Linear(64, 32)\n",
        "        self.fc3 = torch.nn.Linear(32, 2)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.softmax = torch.nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return self.softmax(x)\n",
        "\n",
        "# Load the trained model\n",
        "model = SquatNet()\n",
        "model.load_state_dict(torch.load(r'C:\\Users\\Kassimi\\OneDrive\\Bureau\\cv_data\\squat_model.pth'))\n",
        "model.eval()\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df = pd.read_csv(r\"C:\\Users\\Kassimi\\OneDrive\\Bureau\\cv_data\\squat\\squat_pose_data.csv\")\n",
        "X = df.drop(columns=['label'])\n",
        "scaler.fit(X)\n",
        "\n",
        "# Load YOLOv8 pose model\n",
        "yolo_model = YOLO('yolov8n-pose.pt')\n",
        "\n",
        "\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "if not cap.isOpened():\n",
        "    print(\"❌ Cannot open webcam\")\n",
        "    exit()\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        print(\"❌ Can't receive frame. Exiting...\")\n",
        "        break\n",
        "\n",
        "    # Pose estimation\n",
        "    results = yolo_model.predict(frame, save=False, verbose=False)\n",
        "    keypoints = results[0].keypoints.xy.cpu().numpy()\n",
        "\n",
        "    if len(keypoints) > 0:\n",
        "        # Only first detected person\n",
        "        person_keypoints = keypoints[0].flatten()\n",
        "\n",
        "        if len(person_keypoints) == 34:  # 17 keypoints * 2 (x, y)\n",
        "            # Scale keypoints\n",
        "            person_keypoints_scaled = scaler.transform([person_keypoints])\n",
        "\n",
        "            # Convert to tensor\n",
        "            input_tensor = torch.tensor(person_keypoints_scaled, dtype=torch.float32)\n",
        "\n",
        "            # Predict\n",
        "            with torch.no_grad():\n",
        "                output = model(input_tensor)\n",
        "                _, predicted_class = torch.max(output, 1)\n",
        "\n",
        "            # Map prediction\n",
        "            label = 'Correct' if predicted_class.item() == 1 else 'Wrong'\n",
        "\n",
        "            # Display result on frame\n",
        "            color = (0, 255, 0) if label == 'Correct' else (0, 0, 255)\n",
        "            cv2.putText(frame, label, (30, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.5, color, 3)\n",
        "\n",
        "    # Show frame\n",
        "    cv2.imshow('Live Squat Form Prediction', frame)\n",
        "\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39c05aee-cd64-4c55-a549-4535ddfd5f9a",
      "metadata": {
        "id": "39c05aee-cd64-4c55-a549-4535ddfd5f9a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "# Load CSV\n",
        "df = pd.read_csv(r\"C:\\Users\\Kassimi\\OneDrive\\Bureau\\cv_data\\squat\\squat_pose_data.csv\")\n",
        "\n",
        "# Convert label to numeric\n",
        "df['label'] = df['label'].map({'correct_form': 1, 'wrong_form': 0})\n",
        "\n",
        "# Parameters\n",
        "sequence_length = 30  # Choose fixed sequence length\n",
        "min_frames_required = sequence_length\n",
        "\n",
        "# Normalize keypoints\n",
        "keypoint_cols = [col for col in df.columns if col.startswith('kp_')]\n",
        "scaler = StandardScaler()\n",
        "df[keypoint_cols] = scaler.fit_transform(df[keypoint_cols])\n",
        "\n",
        "# Group by video and build sequences\n",
        "X_sequences = []\n",
        "y_labels = []\n",
        "\n",
        "for video_id, group in df.groupby('video_id'):\n",
        "    group = group.sort_values('frame')\n",
        "    keypoints = group[keypoint_cols].values  # shape: (num_frames, 34)\n",
        "    label = group['label'].iloc[0]\n",
        "\n",
        "    # Skip short sequences\n",
        "    if len(keypoints) < min_frames_required:\n",
        "        continue\n",
        "\n",
        "    # Break long videos into multiple sequences\n",
        "    for start in range(0, len(keypoints) - sequence_length + 1, sequence_length):\n",
        "        seq = keypoints[start:start + sequence_length]\n",
        "        X_sequences.append(seq)\n",
        "        y_labels.append(label)\n",
        "\n",
        "# Convert to tensors\n",
        "\n",
        "\n",
        "X_tensor = torch.tensor(np.array(X_sequences), dtype=torch.float32)\n",
        " # shape: (N, seq_len, 34)\n",
        "y_tensor = torch.tensor(y_labels, dtype=torch.long)\n",
        "\n",
        "# Train/val split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
        "\n",
        "# PyTorch Dataset\n",
        "class SquatSequenceDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# Dataloaders\n",
        "train_dataset = SquatSequenceDataset(X_train, y_train)\n",
        "val_dataset = SquatSequenceDataset(X_val, y_val)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1df01e66-1828-4597-b86d-3b9635f36e27",
      "metadata": {
        "id": "1df01e66-1828-4597-b86d-3b9635f36e27"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class SquatNetLSTM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SquatNetLSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size=34, hidden_size=64, num_layers=2, batch_first=True)\n",
        "        self.fc1 = nn.Linear(64, 64)  # LSTM output to hidden layer\n",
        "        self.fc2 = nn.Linear(64, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, sequence_length, 34)\n",
        "        lstm_out, _ = self.lstm(x)              # Output shape: (batch_size, sequence_length, hidden_size)\n",
        "        x = lstm_out[:, -1, :]                  # Take output from the last time step\n",
        "        x = self.relu(self.fc1(x))              # Pass through FC layer\n",
        "        x = self.fc2(x)                         # Output layer\n",
        "        return self.sigmoid(x)                  # Softmax for probabilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "561c5c8b-f5ea-40f7-8b92-5fcfae548322",
      "metadata": {
        "id": "561c5c8b-f5ea-40f7-8b92-5fcfae548322"
      },
      "outputs": [],
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = SquatNetLSTM().to(device)\n",
        "\n",
        "# Use BCEWithLogitsLoss for binary classification\n",
        "criterion = nn.BCEWithLogitsLoss()  # This combines sigmoid and binary cross-entropy\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.squeeze(), labels.float())  # Ensure labels are of shape (batch_size,)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * inputs.size(0)\n",
        "        predicted = (torch.sigmoid(outputs) > 0.5).float()  # Convert logits to binary predictions\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted.squeeze() == labels).sum().item()\n",
        "\n",
        "    avg_train_loss = train_loss / total\n",
        "    train_acc = correct / total\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs.squeeze(), labels.float())\n",
        "\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted.squeeze() == labels).sum().item()\n",
        "\n",
        "    avg_val_loss = val_loss / total\n",
        "    val_acc = correct / total\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
        "          f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
        "          f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "912a268f-e255-4047-af35-681538e2f30e",
      "metadata": {
        "id": "912a268f-e255-4047-af35-681538e2f30e"
      },
      "outputs": [],
      "source": [
        "# Save the trained model\n",
        "model_path = r\"C:\\Users\\Kassimi\\OneDrive\\Bureau\\cv_data\\squat_lstm_model.pth\"\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"✅ Model saved to {model_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eef073e4-45fb-4b98-acbd-85f8bc77ffa3",
      "metadata": {
        "id": "eef073e4-45fb-4b98-acbd-85f8bc77ffa3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from collections import deque\n",
        "\n",
        "# LSTM sequence length\n",
        "sequence_length = 30\n",
        "\n",
        "# Load trained model\n",
        "model = SquatNetLSTM()\n",
        "model.load_state_dict(torch.load(r'C:\\Users\\Kassimi\\OneDrive\\Bureau\\cv_data\\squat_lstm_model.pth'))\n",
        "model.eval()\n",
        "\n",
        "# Load scaler (refit on original dataset)\n",
        "df = pd.read_csv(r\"C:\\Users\\Kassimi\\OneDrive\\Bureau\\cv_data\\squat\\squat_pose_data.csv\")\n",
        "keypoint_cols = [col for col in df.columns if col.startswith('kp_')]\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(df[keypoint_cols])  # Fit only on keypoints\n",
        "\n",
        "# Load YOLO pose model\n",
        "yolo_model = YOLO('yolov8n-pose.pt')\n",
        "\n",
        "# Video path\n",
        "video_path = r\"C:\\Users\\Kassimi\\OneDrive\\Bureau\\cv_data\\squat\\preprocessed\\wrong\\wrong_4_augmented_6.mp4\"\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Buffer to store sequence of keypoints\n",
        "sequence_buffer = deque(maxlen=sequence_length)\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    results = yolo_model.predict(frame, save=False, verbose=False)\n",
        "    keypoints = results[0].keypoints.xy.cpu().numpy()\n",
        "\n",
        "    if len(keypoints) == 0:\n",
        "        continue\n",
        "\n",
        "    person_keypoints = keypoints[0].flatten()\n",
        "\n",
        "    if len(person_keypoints) != 34:\n",
        "        continue\n",
        "\n",
        "    # Scale keypoints\n",
        "    person_keypoints_scaled = scaler.transform([person_keypoints])[0]\n",
        "    sequence_buffer.append(person_keypoints_scaled)\n",
        "\n",
        "    # Only predict when we have a full sequence\n",
        "    if len(sequence_buffer) == sequence_length:\n",
        "        input_seq = torch.tensor([list(sequence_buffer)], dtype=torch.float32)  # Shape: (1, seq_len, 34)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(input_seq)\n",
        "            _, predicted_class = torch.max(output, 1)\n",
        "\n",
        "        label = 'Correct' if predicted_class.item() == 1 else 'Wrong'\n",
        "        color = (0, 255, 0) if label == 'Correct' else (0, 0, 255)\n",
        "\n",
        "        cv2.putText(frame, label, (30, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.5, color, 3)\n",
        "\n",
        "    cv2.imshow('Squat Form Prediction', frame)\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}